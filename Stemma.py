import streamlit as st
import pandas as pd

# Title and subtitle
st.title("Twi Word Stemmatizer")
st.subheader("Enter a Twi word (like '3y3' or 'bc') to find its stem.")

# Replace keyboard-friendly Twi characters with actual ones
def normalize_twi_input(word):
    return word.replace("3", "…õ").replace("c", "…î")

# Basic stemmer (this is a placeholder rule-based method)
def stem_twi_word(word):
    suffixes = ['mu', 'fo…î', 'ni', 'no', 's…õm', 'fo', 'y…õ', 're', 'ka', 'di', 'to', 'de', 'ma', 'ne', 'na']
    for suffix in suffixes:
        if word.endswith(suffix) and len(word) > len(suffix) + 1:
            return word[:-len(suffix)]
    return word

# The 300 Twi words list (You can expand or customize this list)
twi_words = [
    '…õda', '…õd…î', '…õno', '…õy…õ', '…õreba', '…õk…î', '…õw…î', '…õb…õ', '…õfa', '…õka',
    '…îba', '…îd…î', '…îy…õ', '…îreba', '…îk…î', '…îw…î', '…îb…õ', '…îfa', '…îka', '…îman',
    'abofra', 'abusua', 'adan', 'adaka', 'ad…îe…õ', 'afuo', 'agoro', 'agya', 'ah…îho…î', 'ahenni',
    'akoa', 'ak…îdaa', 'ak…înn…î', 'amane…õ', 'anan', 'as…õm', 'as…îre', 'atade…õ', 'at…õky…õ', 'aware…õ',
    'banb…î', 'barima', 'bere', 'bisa', 'bo…î', 'bra', 'br…õ', 'da', 'd…î', 'dwene',
    'dwom', 'dze', '…õban', '…õb…î', '…õda', '…õd…î', '…õka', '…õkwan', '…õk…î', '…õno',
    '…õre', '…õs…õ', '…õsono', '…õy…õ', 'fa', 'fie', 'foforo', 'fr…õ', 'gya', 'gye',
    'h…î', 'hw…õ', 'ka', 'kasa', 'k…î', 'kyer…õ', 'ky…õ', 'ma', 'man', 'me',
    'mmara', 'mmere', 'mmer…õ', 'mpa', 'mpaboa', 'mpeaw', 'mu', 'na', 'nea', 'ne…õma',
    'nky…õn', 'nkwa', 'nk…îso…î', 'nsa', 'nsuo', 'ns…õm', 'nwanwa', 'nwoma', 'nyansa', 'nya',
    'ny…õ', 'nyimpa', '…îbarima', '…îbea', '…îb…îade…õ', '…îdom', '…îk…îm', '…îman', '…îpanyin', '…îs…îfo…î',
    '…îtan', '…îw…î', '…îy…õ', 'pa', 'papa', 'saa', 's…õ', 's…õnea', 's…õnkanee', 'soma',
    's…î', 's…îre', 'sua', 'suban', 's…õnkanee', 's…õnea', 'te', 'to', 'tumi', 'w…î',
    'w…îfa', 'w…îy…õ', 'wo', 'y…õ', 'yere', 'y…õn', 'y…õre', 'y…õy…õ', 'y…õfo', 'y…õni',
    'y…õreba', 'y…õb…õ', 'y…õda', 'y…õd…î', 'y…õka', 'y…õnko', 'y…õreba', 'y…õreko', 'y…õy…õ', 'y…õw…î',
    'y…õwo', 'y…õy…õfo', 'y…õy…õni', 'y…õy…õre', 'y…õy…õs…õ', 'y…õy…õy…õ', 'y…õy…õb…õ', 'y…õy…õda', 'y…õy…õd…î', 'y…õy…õka',
    'y…õy…õnko', 'y…õy…õreba', 'y…õy…õreko', 'y…õy…õy…õ', 'y…õy…õw…î', 'y…õy…õwo', 'y…õy…õy…õfo', 'y…õy…õy…õni', 'y…õy…õy…õre', 'y…õy…õy…õs…õ',
    'y…õy…õy…õy…õ', 'y…õy…õy…õb…õ', 'y…õy…õy…õda', 'y…õy…õy…õd…î', 'y…õy…õy…õka', 'y…õy…õy…õnko', 'y…õy…õy…õreba', 'y…õy…õy…õreko', 'y…õy…õy…õy…õ', 'y…õy…õy…õw…î',
    'y…õy…õy…õwo', 'y…õy…õy…õy…õfo', 'y…õy…õy…õy…õni', 'y…õy…õy…õy…õre', 'y…õy…õy…õy…õs…õ', 'y…õy…õy…õy…õy…õ', 'y…õy…õy…õy…õb…õ', 'y…õy…õy…õy…õda', 'y…õy…õy…õy…õd…î', 'y…õy…õy…õy…õka'
]

# Apply normalization to word list for matching
normalized_word_map = {normalize_twi_input(word): word for word in twi_words}

# INPUT
user_input = st.text_input("Enter Twi word (type 3 for …õ and c for …î):")
if user_input:
    normalized = normalize_twi_input(user_input.lower())
    if normalized in normalized_word_map:
        stemmed = stem_twi_word(normalized)
        st.success(f"**Original Word:** {normalized_word_map[normalized]}\n\n**Stem:** {stemmed}")
    else:
        st.error("Word not found in dictionary. Please check spelling or use 3 and c as needed.")

# FULL DICTIONARY
with st.expander("üìö View All 300 Twi Words"):
    df = pd.DataFrame({'Available Twi Words': sorted(twi_words)})
    st.dataframe(df, height=300)
